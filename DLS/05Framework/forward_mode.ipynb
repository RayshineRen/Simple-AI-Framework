{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下面为例：\n",
    "\n",
    "$$\n",
    "f(x1,x2)=ln(x1)+x1x2−sin(x2) \\tag{1}\n",
    "$$\n",
    "\n",
    "因为是基于 ADTangent 类进行了操作符重载，因此在初始化自变量 x 和 y 的值需要使用 ADTangent 来初始化，然后通过代码 f = ADTangent.log(x) + x * y - ADTangent.sin(y) 来实现。\n",
    "\n",
    "由于这里是求 f 关于自变量 x 的导数，因此初始化数据的时候呢，自变量 x 的 dx 设置为1，而自变量 y 的 dx 设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADTangent:\n",
    "    def __init__(self, x, dx):\n",
    "        self.x = x\n",
    "        self.dx = dx\n",
    "\n",
    "    def __str__(self):\n",
    "        context = f'value: {self.x:.4f}, grad:{self.dx:.4f}'\n",
    "        return context\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x + other.x\n",
    "            dx = self.dx + other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x + other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            return NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x - other.x\n",
    "            dx = self.dx - other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x - other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            return NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x * other.x\n",
    "            dx = self.x * other.dx + self.dx * other.x\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x * other\n",
    "            dx = self.dx * other\n",
    "        else:\n",
    "            return NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def log(self):\n",
    "        x = np.log(self.x)\n",
    "        dx = self.dx / self.x\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def sin(self):\n",
    "        x = np.sin(self.x)\n",
    "        dx = self.dx * np.cos(self.x)\n",
    "        return ADTangent(x, dx)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 11.6521, grad:5.5000\n"
     ]
    }
   ],
   "source": [
    "x1 = ADTangent(x=2., dx=1)\n",
    "x2 = ADTangent(x=5., dx=0)\n",
    "f = ADTangent.log(x1) + x1 * x2 - ADTangent.sin(x2)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.6521, grad_fn=<SubBackward0>)\n",
      "tensor(5.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.tensor(2.), requires_grad=True)\n",
    "y = Variable(torch.tensor(5.), requires_grad=True)\n",
    "f = torch.log(x) + x * y - torch.sin(y)\n",
    "f.backward()\n",
    "\n",
    "print(f)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(648955:140529887967040,MainProcess):2024-03-24-20:40:28.882.210 [mindspore/run_check/_check_version.py:102] MindSpore version 2.2.11 and cuda version 11.4.148 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.652072]\n",
      "5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor, Parameter\n",
    "\n",
    "class Fun(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Fun, self).__init__()\n",
    "\n",
    "    def construct(self, x, y):\n",
    "        f = ops.log(x) + x * y - ops.sin(y)\n",
    "        return f\n",
    "    \n",
    "x = Tensor(np.array([2.], np.float32))\n",
    "y = Tensor(np.array([5.], np.float32))\n",
    "f = Fun()(x, y)\n",
    "grad_all = ops.GradOperation()\n",
    "grad = grad_all(Fun())(x, y)\n",
    "\n",
    "print(f)\n",
    "print(grad[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
