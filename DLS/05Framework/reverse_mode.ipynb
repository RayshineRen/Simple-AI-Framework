{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NamedTuple, Callable, Dict, Optional\n",
    "import numpy as np\n",
    "\n",
    "_name = 1\n",
    "def fresh_name():\n",
    "    global _name\n",
    "    name = f'v{_name}'\n",
    "    _name += 1\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fresh_name` 用于打印跟 `tape` 相关的变量，并用 `_name` 来记录是第几个变量。\n",
    "\n",
    "为了能够更好滴理解反向模式自动微分的实现，实现代码过程中不依赖PyTorch的autograd。代码中添加了变量类 `Variable` 来跟踪计算梯度，并添加了梯度函数 `grad()` 来计算梯度。\n",
    "\n",
    "对于标量损失l来说，程序中计算的每个张量 x 的值，都会计算值dl/dX。反向模式从 dl/dl=1 开始，使用偏导数和链式规则向后传播导数，例如：\n",
    "\n",
    "$$\n",
    "dl/dx*dx/dy=dl/dy\n",
    "$$\n",
    "\n",
    "下面就是具体的实现过程，首先我们所有的操作都是通过Python进行操作符重载的，而操作符重载，通过 `Variable` 来封装跟踪计算的 Tensor。每个变量都有一个全局唯一的名称 `fresh_name`，因此可以在字典中跟踪该变量的梯度。为了便于理解，`__init__` 有时会提供此名称作为参数。否则，每次都会生成一个新的临时值。\n",
    "\n",
    "为了适配上面图中的简单计算，这里面只提供了 乘、加、减、sin、log 五种计算方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value, name=None):\n",
    "        self.value = value\n",
    "        self.name = name or fresh_name()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def constant(value, name=None):\n",
    "        var = Variable(value, name=name)\n",
    "        print(f'{var.name} = {value}')\n",
    "        return var\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return ops_add(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return ops_mul(self, other)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return ops_sub(self, other)\n",
    "    \n",
    "    def sin(self):\n",
    "        return ops_sin(self)\n",
    "    \n",
    "    def log(self):\n",
    "        return ops_log(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tape(NamedTuple):\n",
    "    inputs : List[str]\n",
    "    outputs : List[str]\n",
    "    propagate : 'Callable[List[Variable], List[Variable]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入 `inputs` 和输出 `outputs` 是原始计算的输入和输出变量的唯一名称。反向传播使用链式规则，将函数的输出梯度传播给输入。其输入为 dL/dOutputs，输出为 dL/dinput。Tape只是一个记录所有计算的累积 List 列表。\n",
    "\n",
    "下面提供了一种重置 Tape 的方法 `reset_tape`，方便运行多次自动微分，每次自动微分过程都会产生 Tape List。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_tape : List[Tape] = []\n",
    "\n",
    "def reset_tape():\n",
    "    global _name\n",
    "    _name = 1\n",
    "    gradient_tape.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops_mul(self, other):\n",
    "    x = Variable(self.value * other.value)\n",
    "    print(f'{x.name} = {self.name} * {other.name}')\n",
    "\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = other\n",
    "        dx_dother = self\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        dl_dinputs = [dl_dself, dl_dother]\n",
    "        return dl_dinputs\n",
    "    \n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops_add(self, other):\n",
    "    x = Variable(self.value + other.value)\n",
    "    print(f'{x.name} = {self.name} + {other.name}')\n",
    "\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0)\n",
    "        dx_dother = Variable(1.0)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        dl_dinputs = [dl_dself, dl_dother]\n",
    "        return dl_dinputs\n",
    "    \n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops_sub(self, other):\n",
    "    x = Variable(self.value - other.value)\n",
    "    print(f'{x.name} = {self.name} - {other.name}')\n",
    "\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0)\n",
    "        dx_dother = Variable(-1.0)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        dl_dinputs = [dl_dself, dl_dother]\n",
    "        return dl_dinputs\n",
    "    \n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops_sin(self):\n",
    "    x = Variable(np.sin(self.value))\n",
    "    print(f'{x.name} = sin({self.name})')\n",
    "\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(np.cos(self.value))\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dinputs = [dl_dself]\n",
    "        return dl_dinputs\n",
    "    \n",
    "    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops_log(self):\n",
    "    x = Variable(np.log(self.value))\n",
    "    print(f'{x.name} = log({self.name})')\n",
    "\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0 / self.value)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dinputs = [dl_dself]\n",
    "        return dl_dinputs\n",
    "\n",
    "    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(l, results):\n",
    "    dl_d = {}\n",
    "    dl_d[l.name] = Variable(1.0)\n",
    "    print(\"dl_d\", dl_d)\n",
    "\n",
    "    def gather_grad(entries):\n",
    "        return [dl_d[entry] if entry in dl_d else None for entry in entries]\n",
    "    \n",
    "    for entry in reversed(gradient_tape):\n",
    "        print(entry)\n",
    "        dl_doutputs = gather_grad(entry.outputs)\n",
    "        dl_dinputs = entry.propagate(dl_doutputs)\n",
    "\n",
    "        for input, dl_dinput in zip(entry.inputs, dl_dinputs):\n",
    "            if input not in dl_d:\n",
    "                dl_d[input] = dl_dinput\n",
    "            else:\n",
    "                dl_d[input] += dl_dinput\n",
    "\n",
    "    for name, value in dl_d.items():\n",
    "        print(f'dl_d{name} = {value.value}')\n",
    "\n",
    "    return gather_grad(result.name for result in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v-1 = 2.0\n",
      "v0 = 5.0\n",
      "v1 = log(v-1)\n",
      "v2 = v-1 * v0\n",
      "v3 = v1 + v2\n",
      "v4 = sin(v0)\n",
      "v5 = v3 - v4\n",
      "11.652071455223084\n"
     ]
    }
   ],
   "source": [
    "reset_tape()\n",
    "\n",
    "x = Variable.constant(2.0, name=\"v-1\")\n",
    "y = Variable.constant(5.0, name=\"v0\")\n",
    "\n",
    "f = Variable.log(x) + x * y - Variable.sin(y)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_d {'v5': 1.0}\n",
      "Tape(inputs=['v3', 'v4'], outputs=['v5'], propagate=<function ops_sub.<locals>.propagate at 0x7fbd98b24a60>)\n",
      "v9 = v6 * v7\n",
      "v10 = v6 * v8\n",
      "Tape(inputs=['v0'], outputs=['v4'], propagate=<function ops_sin.<locals>.propagate at 0x7fbd98b249d0>)\n",
      "v12 = v10 * v11\n",
      "Tape(inputs=['v1', 'v2'], outputs=['v3'], propagate=<function ops_add.<locals>.propagate at 0x7fbd98b24940>)\n",
      "v15 = v9 * v13\n",
      "v16 = v9 * v14\n",
      "Tape(inputs=['v-1', 'v0'], outputs=['v2'], propagate=<function ops_mul.<locals>.propagate at 0x7fbd98b248b0>)\n",
      "v17 = v16 * v0\n",
      "v18 = v16 * v-1\n",
      "v19 = v12 + v18\n",
      "Tape(inputs=['v-1'], outputs=['v1'], propagate=<function ops_log.<locals>.propagate at 0x7fbd98b24820>)\n",
      "v21 = v15 * v20\n",
      "v22 = v17 + v21\n",
      "dl_dv5 = 1.0\n",
      "dl_dv3 = 1.0\n",
      "dl_dv4 = -1.0\n",
      "dl_dv0 = 1.7163378145367738\n",
      "dl_dv1 = 1.0\n",
      "dl_dv2 = 1.0\n",
      "dl_dv-1 = 5.5\n",
      "dx 5.5\n",
      "dy 1.7163378145367738\n"
     ]
    }
   ],
   "source": [
    "dx, dy = grad(f, [x, y])\n",
    "print(\"dx\", dx)\n",
    "print(\"dy\", dy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
